{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fc63b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac269b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_predictions(probas): \n",
    "    \"\"\"\n",
    "    Create mean predictions\n",
    "    :param probas: 2-d array of probability values :return: mean probability\n",
    "    \"\"\"\n",
    "    return np.mean(probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f356ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_voting(preds): \n",
    "    \"\"\"\n",
    "    Create mean predictions\n",
    "    :param probas: 2-d array of prediction values\n",
    "    :return: max voted predictions\n",
    "    \"\"\"\n",
    "    idxs = np.argmax(preds, axis=1)\n",
    "    return np.take_along_axis(preds, idxs[:, None], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efa673cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_mean(probas): \n",
    "    \"\"\"\n",
    "    Create mean predictions using ranks\n",
    "    :param probas: 2-d array of probability values :return: mean ranks\n",
    "    \"\"\"\n",
    "    ranked = []\n",
    "    for i in range(probas.shape[1]):\n",
    "        rank_data = stats.rankdata(probas[:, i]) \n",
    "        ranked.append(rank_data)\n",
    "    ranked = np.column_stack(ranked)\n",
    "    return np.mean(ranked, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f212fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = np.array([\n",
    "    [0.1, 0.2],\n",
    "    [0.4, 0.3],\n",
    "    [0.35, 0.5]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf7100a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 2.5, 2.5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_mean(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a4c1a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1 , 0.4 , 0.35])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82a710a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.3, 0.5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae3329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c45c0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from scipy.optimize import fmin\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "class OptimizeAUC:\n",
    "    \"\"\"\n",
    "    Class for optimizing AUC.\n",
    "    This class is all you need to find best weights for\n",
    "    any model and for any metric and for any types of predictions.\n",
    "    With very small changes, this class can be used for optimization of\n",
    "    weights in ensemble models of _any_ type of predictions\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _auc(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        This functions calulates and returns AUC.\n",
    "        :param coef: coef list, of the same length as number of models\n",
    "        :param X: predictions, in this case a 2d array\n",
    "        :param y: targets, in our case binary 1d array\n",
    "        \"\"\"\n",
    "        # multiply coefficients with every column of the array with predictions.\n",
    "        # this means: element 1 of coef is multiplied by column 1\n",
    "        # of the prediction array, element 2 of coef is multiplied\n",
    "        # by column 2 of the prediction array and so on!\n",
    "        x_coef = X * coef\n",
    "\n",
    "        # create predictions by taking row wise sum\n",
    "        predictions = np.sum(x_coef, axis=1)\n",
    "\n",
    "        # calculate auc score\n",
    "        auc_score = metrics.roc_auc_score(y, predictions)\n",
    "\n",
    "        # return negative auc\n",
    "        return -1.0 * auc_score\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # remember partial from hyperparameter optimization chapter?\n",
    "        loss_partial = partial(self._auc, X=X, y=y)\n",
    "\n",
    "        # dirichlet distribution. you can use any distribution you want\n",
    "        # to initialize the coefficients\n",
    "        # we want the coefficients to sum to 1\n",
    "        initial_coef = np.random.dirichlet(np.ones(X.shape[1]), size=1)\n",
    "\n",
    "        # use scipy fmin to minimize the loss function, in our case auc\n",
    "        self.coef_ = fmin(loss_partial, initial_coef, disp=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # this is similar to _auc function\n",
    "        x_coef = X * self.coef_\n",
    "        predictions = np.sum(x_coef, axis=1)\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55d8bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50a358c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a binary classification dataset with 10k samples\n",
    "# and 25 features\n",
    "X, y = make_classification(n_samples=10000, n_features=25)\n",
    "\n",
    "# split into two folds (for this example)\n",
    "xfold1, xfold2, yfold1, yfold2 = model_selection.train_test_split(X,\n",
    "                                                                  y, test_size=0.5, stratify=y\n",
    "                                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f030f8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.17487004,  0.65548221, -0.47785476, ...,  0.90527309,\n",
       "        -0.57431901, -0.36933863],\n",
       "       [-0.4760209 , -1.60754802,  0.22496259, ...,  1.58323044,\n",
       "        -1.10087941, -0.24167795],\n",
       "       [ 0.29474326,  0.79031379,  3.39788006, ...,  0.89997396,\n",
       "        -0.84451195,  0.77934592],\n",
       "       ...,\n",
       "       [ 0.55586449,  0.38940475, -1.79915524, ..., -1.18499049,\n",
       "         1.1146075 , -1.03723971],\n",
       "       [-1.37516793,  0.2134832 , -0.8950755 , ...,  0.26862395,\n",
       "        -0.41450558,  0.91342018],\n",
       "       [-0.95469001, -1.01887457,  0.90601411, ...,  0.5176188 ,\n",
       "        -0.69597536,  1.32946223]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xfold1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77680867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit models on fold 1 and make predictions on fold 2\n",
    "# we have 3 models:\n",
    "# logistic regression, random forest and xgboost\n",
    "logreg = linear_model.LogisticRegression()\n",
    "rf = ensemble.RandomForestClassifier()\n",
    "xgbc = xgb.XGBClassifier()\n",
    "\n",
    "# fit all models on fold 1 data\n",
    "logreg.fit(xfold1, yfold1)\n",
    "rf.fit(xfold1, yfold1)\n",
    "xgbc.fit(xfold1, yfold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe18c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all models on fold 2\n",
    "# take probability for class 1\n",
    "pred_logreg = logreg.predict_proba(xfold2)[:, 1]\n",
    "pred_rf = rf.predict_proba(xfold2)[:, 1]\n",
    "pred_xgbc = xgbc.predict_proba(xfold2)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aa0ac75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict_proba(xfold2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d232653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an average of all predictions\n",
    "# that is the simplest ensemble\n",
    "avg_pred = (pred_logreg + pred_rf + pred_xgbc) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d7359492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 2d array of all predictions\n",
    "fold2_preds = np.column_stack((pred_logreg,\n",
    "                                pred_rf,\n",
    "                                pred_xgbc,\n",
    "                                avg_pred\n",
    "                                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd4a1bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold2_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "acc5c6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98228548, 0.95      , 0.99819142, 0.97682563],\n",
       "       [0.81878213, 0.96      , 0.99504292, 0.92460835],\n",
       "       [0.2240907 , 0.23      , 0.08329701, 0.17912924],\n",
       "       ...,\n",
       "       [0.99936661, 0.96      , 0.99968922, 0.98635195],\n",
       "       [0.77698095, 0.9       , 0.99749315, 0.89149137],\n",
       "       [0.00384836, 0.08      , 0.00482325, 0.0295572 ]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold2_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "29490023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-2: LR AUC = 0.9670788799999999\n",
      "Fold-2: RF AUC = 0.98100432\n",
      "Fold-2: XGB AUC = 0.98042384\n",
      "Fold-2: Average Pred AUC = 0.97925504\n"
     ]
    }
   ],
   "source": [
    "# calculate and store individual AUC values\n",
    "aucs_fold2 = []\n",
    "for i in range(fold2_preds.shape[1]):\n",
    "    auc = metrics.roc_auc_score(yfold2, fold2_preds[:, i])\n",
    "    aucs_fold2.append(auc)\n",
    "print(f\"Fold-2: LR AUC = {aucs_fold2[0]}\")\n",
    "print(f\"Fold-2: RF AUC = {aucs_fold2[1]}\")\n",
    "print(f\"Fold-2: XGB AUC = {aucs_fold2[2]}\")\n",
    "print(f\"Fold-2: Average Pred AUC = {aucs_fold2[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a7bccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1: LR AUC = 0.9632784000000001\n",
      "Fold-1: RF AUC = 0.97840488\n",
      "Fold-1: XGB AUC = 0.9786933600000001\n",
      "Fold-1: Average prediction AUC = 0.97658672\n"
     ]
    }
   ],
   "source": [
    "# now we repeat the same for the other fold\n",
    "# this is not the ideal way, if you ever have to repeat code,\n",
    "# create a function!\n",
    "# fit models on fold 2 and make predictions on fold 1\n",
    "logreg = linear_model.LogisticRegression()\n",
    "rf = ensemble.RandomForestClassifier()\n",
    "xgbc = xgb.XGBClassifier()\n",
    "\n",
    "logreg.fit(xfold2, yfold2)\n",
    "rf.fit(xfold2, yfold2)\n",
    "xgbc.fit(xfold2, yfold2)\n",
    "\n",
    "pred_logreg = logreg.predict_proba(xfold1)[:, 1]\n",
    "pred_rf = rf.predict_proba(xfold1)[:, 1]\n",
    "pred_xgbc = xgbc.predict_proba(xfold1)[:, 1]\n",
    "avg_pred = (pred_logreg + pred_rf + pred_xgbc) / 3\n",
    "\n",
    "fold1_preds = np.column_stack(( pred_logreg,\n",
    "                                pred_rf,\n",
    "                                pred_xgbc,\n",
    "                                avg_pred\n",
    "                                ))\n",
    "\n",
    "aucs_fold1 = []\n",
    "for i in range(fold1_preds.shape[1]):\n",
    "    auc = metrics.roc_auc_score(yfold1, fold1_preds[:, i])\n",
    "    aucs_fold1.append(auc)\n",
    "\n",
    "print(f\"Fold-1: LR AUC = {aucs_fold1[0]}\")\n",
    "print(f\"Fold-1: RF AUC = {aucs_fold1[1]}\")\n",
    "print(f\"Fold-1: XGB AUC = {aucs_fold1[2]}\")\n",
    "print(f\"Fold-1: Average prediction AUC = {aucs_fold1[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8abbaf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal weights using the optimizer\n",
    "opt = OptimizeAUC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "481dc92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.89664496e-02, 1.20000000e-01, 3.57584562e-04, 4.97746780e-02],\n",
       "       [1.79099851e-02, 6.00000000e-02, 1.65189465e-03, 2.65206266e-02],\n",
       "       [9.69476399e-01, 9.90000000e-01, 9.99962807e-01, 9.86479735e-01],\n",
       "       ...,\n",
       "       [1.96954088e-02, 1.00000000e-02, 2.91284614e-05, 9.90817907e-03],\n",
       "       [9.89492760e-01, 9.90000000e-01, 9.99889731e-01, 9.93127497e-01],\n",
       "       [9.99114508e-01, 1.00000000e+00, 9.99945045e-01, 9.99686517e-01]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "46e4a1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold1_preds[:, :-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f1e6892e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.89664496e-02, 1.20000000e-01, 3.57584562e-04],\n",
       "       [1.79099851e-02, 6.00000000e-02, 1.65189465e-03],\n",
       "       [9.69476399e-01, 9.90000000e-01, 9.99962807e-01],\n",
       "       ...,\n",
       "       [1.96954088e-02, 1.00000000e-02, 2.91284614e-05],\n",
       "       [9.89492760e-01, 9.90000000e-01, 9.99889731e-01],\n",
       "       [9.99114508e-01, 1.00000000e+00, 9.99945045e-01]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold1_preds[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e648c41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.978986\n",
      "         Iterations: 55\n",
      "         Function evaluations: 121\n",
      "Optimized AUC, Fold 2 = 0.9809361600000001\n",
      "Coefficients = [-0.08164794  0.72127158  0.68666043]\n"
     ]
    }
   ],
   "source": [
    "# dont forget to remove the average column\n",
    "opt.fit(fold1_preds[:, :-1], yfold1)\n",
    "opt_preds_fold2 = opt.predict(fold2_preds[:, :-1])\n",
    "auc = metrics.roc_auc_score(yfold2, opt_preds_fold2)\n",
    "print(f\"Optimized AUC, Fold 2 = {auc}\")\n",
    "print(f\"Coefficients = {opt.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "21975439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.981360\n",
      "         Iterations: 41\n",
      "         Function evaluations: 91\n",
      "Optimized AUC, Fold 1 = 0.9789088\n",
      "Coefficients = [-3.20702546e-04  9.86797083e-01  4.04664866e-01]\n"
     ]
    }
   ],
   "source": [
    "opt.fit(fold2_preds[:, :-1], yfold2)\n",
    "opt_preds_fold1 = opt.predict(fold1_preds[:, :-1])\n",
    "auc = metrics.roc_auc_score(yfold1, opt_preds_fold1)\n",
    "print(f\"Optimized AUC, Fold 1 = {auc}\")\n",
    "print(f\"Coefficients = {opt.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f4df09d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.20702546e-04,  9.86797083e-01,  4.04664866e-01])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b07de4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 加载数据集\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = (data.target == 2).astype(int)  # 将问题转化为二分类问题\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 训练基础模型\n",
    "model1 = LogisticRegression()\n",
    "model2 = DecisionTreeClassifier()\n",
    "model3 = RandomForestClassifier()\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "# 生成基础模型的预测结果\n",
    "preds1 = model1.predict_proba(X_train)[:, 1]\n",
    "preds2 = model2.predict_proba(X_train)[:, 1]\n",
    "preds3 = model3.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# 将基础模型的预测结果作为元模型的输入特征\n",
    "stacked_features = np.column_stack((preds1, preds2, preds3))\n",
    "\n",
    "# 训练元模型\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(stacked_features, y_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "test_preds1 = model1.predict_proba(X_test)[:, 1]\n",
    "test_preds2 = model2.predict_proba(X_test)[:, 1]\n",
    "test_preds3 = model3.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 将基础模型的预测结果作为元模型的输入特征\n",
    "stacked_test_features = np.column_stack((test_preds1, test_preds2, test_preds3))\n",
    "\n",
    "# 使用元模型进行最终预测\n",
    "final_preds = meta_model.predict(stacked_test_features)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, final_preds)\n",
    "print(\"Stacking Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "06671071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Model Average Accuracy: 0.9533333333333335\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 加载数据集\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = (data.target == 2).astype(int)  # 将问题转化为二分类问题\n",
    "\n",
    "# 定义基础模型\n",
    "model1 = LogisticRegression()\n",
    "model2 = DecisionTreeClassifier()\n",
    "model3 = RandomForestClassifier()\n",
    "\n",
    "# 定义元模型\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# 使用K折交叉验证\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # 训练基础模型\n",
    "    model1.fit(X_train, y_train)\n",
    "    model2.fit(X_train, y_train)\n",
    "    model3.fit(X_train, y_train)\n",
    "    \n",
    "    # 生成基础模型的预测结果\n",
    "    preds1 = model1.predict_proba(X_train)[:, 1]\n",
    "    preds2 = model2.predict_proba(X_train)[:, 1]\n",
    "    preds3 = model3.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # 将基础模型的预测结果作为元模型的输入特征\n",
    "    stacked_features = np.column_stack((preds1, preds2, preds3))\n",
    "    \n",
    "    # 训练元模型\n",
    "    meta_model.fit(stacked_features, y_train)\n",
    "    \n",
    "    # 在测试集上进行预测\n",
    "    test_preds1 = model1.predict_proba(X_test)[:, 1]\n",
    "    test_preds2 = model2.predict_proba(X_test)[:, 1]\n",
    "    test_preds3 = model3.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # 将基础模型的预测结果作为元模型的输入特征\n",
    "    stacked_test_features = np.column_stack((test_preds1, test_preds2, test_preds3))\n",
    "    \n",
    "    # 使用元模型进行最终预测\n",
    "    final_preds = meta_model.predict(stacked_test_features)\n",
    "    \n",
    "    # 计算准确率\n",
    "    accuracy = accuracy_score(y_test, final_preds)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# 输出平均准确率\n",
    "print(\"Stacking Model Average Accuracy:\", np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6981a633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Model Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 加载数据集\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 定义基模型\n",
    "base_models = [\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('svc', SVC(probability=True))\n",
    "]\n",
    "\n",
    "# 定义元学习器\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# 创建 Stacking 模型\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "# 训练 Stacking 模型\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# 评估性能\n",
    "accuracy = stacking_model.score(X_test, y_test)\n",
    "print(f\"Stacking Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0a9c85a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Average Accuracy: 0.9733333333333334\n",
      "Decision Tree Model Average Accuracy: 0.9533333333333335\n",
      "Random Forest Model Average Accuracy: 0.9600000000000002\n",
      "Stacking Model Average Accuracy: 0.9533333333333335\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 加载数据集\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = (data.target == 2).astype(int)  # 将问题转化为二分类问题\n",
    "\n",
    "# 定义基础模型\n",
    "model1 = LogisticRegression()\n",
    "model2 = DecisionTreeClassifier()\n",
    "model3 = RandomForestClassifier()\n",
    "\n",
    "# 定义元模型\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# 使用K折交叉验证\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 存储每个模型的准确率\n",
    "model1_accuracies = []\n",
    "model2_accuracies = []\n",
    "model3_accuracies = []\n",
    "stacking_accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # 训练基础模型\n",
    "    model1.fit(X_train, y_train)\n",
    "    model2.fit(X_train, y_train)\n",
    "    model3.fit(X_train, y_train)\n",
    "    \n",
    "    # 生成基础模型的预测结果\n",
    "    preds1 = model1.predict_proba(X_train)[:, 1]\n",
    "    preds2 = model2.predict_proba(X_train)[:, 1]\n",
    "    preds3 = model3.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # 将基础模型的预测结果作为元模型的输入特征\n",
    "    stacked_features = np.column_stack((preds1, preds2, preds3))\n",
    "    \n",
    "    # 训练元模型\n",
    "    meta_model.fit(stacked_features, y_train)\n",
    "    \n",
    "    # 在测试集上进行预测\n",
    "    test_preds1 = model1.predict_proba(X_test)[:, 1]\n",
    "    test_preds2 = model2.predict_proba(X_test)[:, 1]\n",
    "    test_preds3 = model3.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # 将基础模型的预测结果作为元模型的输入特征\n",
    "    stacked_test_features = np.column_stack((test_preds1, test_preds2, test_preds3))\n",
    "    \n",
    "    # 使用元模型进行最终预测\n",
    "    final_preds = meta_model.predict(stacked_test_features)\n",
    "    \n",
    "    # 计算每个基础模型的准确率\n",
    "    model1_accuracy = accuracy_score(y_test, model1.predict(X_test))\n",
    "    model2_accuracy = accuracy_score(y_test, model2.predict(X_test))\n",
    "    model3_accuracy = accuracy_score(y_test, model3.predict(X_test))\n",
    "    \n",
    "    # 计算Stacking模型的准确率\n",
    "    stacking_accuracy = accuracy_score(y_test, final_preds)\n",
    "    \n",
    "    # 存储准确率\n",
    "    model1_accuracies.append(model1_accuracy)\n",
    "    model2_accuracies.append(model2_accuracy)\n",
    "    model3_accuracies.append(model3_accuracy)\n",
    "    stacking_accuracies.append(stacking_accuracy)\n",
    "\n",
    "# 输出每个模型的平均准确率\n",
    "print(\"Logistic Regression Model Average Accuracy:\", np.mean(model1_accuracies))\n",
    "print(\"Decision Tree Model Average Accuracy:\", np.mean(model2_accuracies))\n",
    "print(\"Random Forest Model Average Accuracy:\", np.mean(model3_accuracies))\n",
    "print(\"Stacking Model Average Accuracy:\", np.mean(stacking_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8afbc8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Average Accuracy: 0.9666666666666668\n",
      "Decision Tree Model Average Accuracy: 0.9533333333333334\n",
      "Random Forest Model Average Accuracy: 0.9600000000000002\n",
      "SVM Model Average Accuracy: 0.9666666666666668\n",
      "Stacking Model Average Accuracy: 0.9400000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 加载数据集\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = (data.target == 2).astype(int)  # 将问题转化为二分类问题\n",
    "\n",
    "# 定义基础模型\n",
    "model1 = LogisticRegression()\n",
    "model2 = DecisionTreeClassifier()\n",
    "model3 = RandomForestClassifier()\n",
    "model4 = SVC(probability=True)  # 新增一个支持向量机模型\n",
    "\n",
    "# 定义元模型\n",
    "meta_model = GradientBoostingClassifier()  # 使用梯度提升决策树作为元模型\n",
    "\n",
    "# 使用K折交叉验证\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 存储每个模型的准确率\n",
    "model1_accuracies = []\n",
    "model2_accuracies = []\n",
    "model3_accuracies = []\n",
    "model4_accuracies = []\n",
    "stacking_accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # 训练基础模型\n",
    "    model1.fit(X_train, y_train)\n",
    "    model2.fit(X_train, y_train)\n",
    "    model3.fit(X_train, y_train)\n",
    "    model4.fit(X_train, y_train)\n",
    "    \n",
    "    # 生成基础模型的预测结果\n",
    "    preds1 = model1.predict_proba(X_train)[:, 1]\n",
    "    preds2 = model2.predict_proba(X_train)[:, 1]\n",
    "    preds3 = model3.predict_proba(X_train)[:, 1]\n",
    "    preds4 = model4.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # 将基础模型的预测结果作为元模型的输入特征\n",
    "    stacked_features = np.column_stack((preds1, preds2, preds3, preds4))\n",
    "    \n",
    "    # 训练元模型\n",
    "    meta_model.fit(stacked_features, y_train)\n",
    "    \n",
    "    # 在测试集上进行预测\n",
    "    test_preds1 = model1.predict_proba(X_test)[:, 1]\n",
    "    test_preds2 = model2.predict_proba(X_test)[:, 1]\n",
    "    test_preds3 = model3.predict_proba(X_test)[:, 1]\n",
    "    test_preds4 = model4.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # 将基础模型的预测结果作为元模型的输入特征\n",
    "    stacked_test_features = np.column_stack((test_preds1, test_preds2, test_preds3, test_preds4))\n",
    "    \n",
    "    # 使用元模型进行最终预测\n",
    "        # 使用元模型进行最终预测\n",
    "    final_preds = meta_model.predict(stacked_test_features)\n",
    "    \n",
    "    # 计算每个基础模型的准确率\n",
    "    model1_accuracy = accuracy_score(y_test, model1.predict(X_test))\n",
    "    model2_accuracy = accuracy_score(y_test, model2.predict(X_test))\n",
    "    model3_accuracy = accuracy_score(y_test, model3.predict(X_test))\n",
    "    model4_accuracy = accuracy_score(y_test, model4.predict(X_test))\n",
    "    \n",
    "    # 计算Stacking模型的准确率\n",
    "    stacking_accuracy = accuracy_score(y_test, final_preds)\n",
    "    \n",
    "    # 存储准确率\n",
    "    model1_accuracies.append(model1_accuracy)\n",
    "    model2_accuracies.append(model2_accuracy)\n",
    "    model3_accuracies.append(model3_accuracy)\n",
    "    model4_accuracies.append(model4_accuracy)\n",
    "    stacking_accuracies.append(stacking_accuracy)\n",
    "\n",
    "# 输出每个模型的平均准确率\n",
    "print(\"Logistic Regression Model Average Accuracy:\", np.mean(model1_accuracies))\n",
    "print(\"Decision Tree Model Average Accuracy:\", np.mean(model2_accuracies))\n",
    "print(\"Random Forest Model Average Accuracy:\", np.mean(model3_accuracies))\n",
    "print(\"SVM Model Average Accuracy:\", np.mean(model4_accuracies))\n",
    "print(\"Stacking Model Average Accuracy:\", np.mean(stacking_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9c78c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
